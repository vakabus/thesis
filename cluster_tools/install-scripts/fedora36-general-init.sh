#!/bin/bash

set -e
export PATH=/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin

# wait for full startup
echo "Waiting for full system startup..."
while [[ "$(systemctl show --property=SystemState)" != "SystemState=running" ]]; do
    sleep 1
done

function setup_dnf_local {
    sudo dnf install -y python3-dnf-plugin-local cifs-utils
    cat <<EOF | sudo tee /etc/dnf/plugins/local.conf
[main]
enabled = true
# Path to the local repository.
repodir = /mnt/dnf-local/repo

# Createrepo options. See man createrepo_c
[createrepo]
# This option lets you disable createrepo command. This could be useful
# for large repositories where metadata is priodically generated by cron
# for example. This also has the side effect of only copying the packages
# to the local repo directory.
enabled = true

# If you want to speedup createrepo with the --cachedir option. Eg.
# cachedir = /tmp/createrepo-local-plugin-cachedir

# quiet = true

# verbose = false
EOF
    sudo mkdir /mnt/dnf-local
    echo "//192.168.1.247/dnf-local /mnt/dnf-local  smb3   guest" | sudo tee -a /etc/fstab
    
    sudo systemctl daemon-reload
    sudo mount -a
}




function install_docker {
    sudo dnf -y install dnf-plugins-core
    sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo

    sudo dnf install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin
    sudo dnf install -y https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.1/cri-dockerd-0.3.1-3.fc36.x86_64.rpm

    sudo systemctl enable --now docker
    sudo systemctl enable --now cri-docker
}

function install_containerd {
    # does not work, but leaving it here for potential future use

    # install runc
    wget https://github.com/opencontainers/runc/releases/download/v1.1.4/runc.amd64
    sudo install -m 755 runc.amd64 /usr/local/sbin/runc

    # install containerd
    wget https://github.com/containerd/containerd/releases/download/v1.6.14/containerd-1.6.14-linux-amd64.tar.gz
    sudo tar Czxvf /usr/local containerd-1.6.14-linux-amd64.tar.gz

    wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service
    sudo mv containerd.service /usr/lib/systemd/system/
    sudo restorecon -Rv /usr/lib/systemd/system/

    sudo mkdir -p /etc/containerd/
    containerd config default | sudo tee /etc/containerd/config.toml
    sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml

    sudo systemctl daemon-reload
    sudo systemctl enable --now containerd
}

function install_kubernetes {
    cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

    # Set SELinux in permissive mode (effectively disabling it)
    sudo setenforce 0
    sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
    sudo yum install -y kubelet kubeadm kubectl kubernetes-cni --disableexcludes=kubernetes
    sudo systemctl enable --now kubelet

    # configure system
    cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

    sudo modprobe overlay
    sudo modprobe br_netfilter

    # sysctl params required by setup, params persist across reboots
    cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

    # Apply sysctl params without reboot
    sudo sysctl --system
}


function setup_sudo_rsync {
    cat <<EOF | sudo tee /usr/local/bin/sudo_rsync
#!/bin/sh

exec sudo rsync \$@
EOF
    sudo chmod +x /usr/local/bin/sudo_rsync
}

function cluster_ip {
    if echo $1 | grep -E -o '^kb[0-9]$' > /dev/null; then
        # the name is kbX
        echo 192.168.1.22${1#"kb"}
    else
        # it's a labs device
        num=$(echo $1 | grep -o -E '[0-9]+')
        echo "192.168.1.22$(($num - 63))"
    fi
}

function configure_systemd_networkd {
    ip="$(cluster_ip $1)"
    # singular NIC at home
    cat <<EOF | sudo tee /etc/systemd/network/eth0.network
[Match]
Name=eth0

[Network]
Address=$ip/24
Gateway=192.168.1.1
DNS=192.168.1.1
EOF

    # management NIC on the big server, just bring it up
    cat <<EOF | sudo tee /etc/systemd/network/eno3.network
[Match]
Name=eno3

[Network]
DHCP=no
IPv6AcceptRA=no
EOF
    cat <<EOF | sudo tee /etc/systemd/network/breno3.network
[Match]
Name=breno3

[Network]
DHCP=ipv4
IPv6AcceptRA=no
EOF

    # cluster NIC on the big server
    cat <<EOF | sudo tee /etc/systemd/network/eno1.network
[Match]
Name=eno1

[Network]
Address=$ip/24
EOF

    sudo systemctl disable NetworkManager
    sudo systemctl mask NetworkManager
    sudo systemctl enable systemd-networkd
}

function configure_network_manager {
    sudo systemctl enable --now openvswitch.service
    sudo systemctl restart NetworkManager # loads ovs plugin

    # cluster interface
    IP_ADDRESS="$(cluster_ip $1)"
    interfaces=$(ip -o link show | awk -F': ' '{print $2}')
    home="false"
    if echo "$interfaces" | grep eth0 > /dev/null; then
        # homelab
        CLUSTER_IF=eth0
        BRIDGE_NAME=breth0
        PUB_IF=eth1
        home="true"
    else
        # big server lab
        CLUSTER_IF=eno1
        BRIDGE_NAME=breno1
        PUB_IF=eno3
    fi

    if ! sudo nmcli conn show ${BRIDGE_NAME} > /dev/null; then
        sudo nmcli c add type ovs-bridge conn.interface ${BRIDGE_NAME} con-name ${BRIDGE_NAME}
        sudo nmcli c add type ovs-port conn.interface ${BRIDGE_NAME} master ${BRIDGE_NAME} con-name ovs-port-${BRIDGE_NAME}
        sudo nmcli c add type ovs-interface slave-type ovs-port conn.interface ${BRIDGE_NAME} master ovs-port-${BRIDGE_NAME}  con-name ovs-if-${BRIDGE_NAME}
        sudo nmcli c add type ovs-port conn.interface ${CLUSTER_IF} master ${BRIDGE_NAME} con-name ovs-port-${CLUSTER_IF}
        sudo nmcli c add type ethernet conn.interface ${CLUSTER_IF} master ovs-port-${CLUSTER_IF} con-name ovs-if-${CLUSTER_IF}
    fi

    sudo nmcli conn mod ${CLUSTER_IF} connection.autoconnect no || true  # DHCP on $PUB_IF
    sudo nmcli conn mod "cloud-init ${CLUSTER_IF}" connection.autoconnect no || true  # DHCP on $PUB_IF

    sudo nmcli conn mod ${BRIDGE_NAME} connection.autoconnect yes
    sudo nmcli conn mod ovs-if-${BRIDGE_NAME} connection.autoconnect yes
    sudo nmcli conn mod ovs-if-${CLUSTER_IF} connection.autoconnect yes
    sudo nmcli conn mod ovs-port-${CLUSTER_IF} connection.autoconnect yes
    sudo nmcli conn mod ovs-port-${BRIDGE_NAME} connection.autoconnect yes

    sudo nmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.address ${IP_ADDRESS}/24
    sudo nmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.method static
    sudo nmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.route-metric 50

    if $home; then
        # with 2 network cards in the VMs, we don't have to configure anything either
        : sudo nmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.gateway "192.168.1.1"
        : sudo nmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.dns "192.168.1.1"
    else
        : # when running in the lab, the routes and connections are already configured correctly
    fi

    # there will be reboot, so we probably don't have to do this
    # sudo nmcli conn reload

    sudo systemctl restart openvswitch

    # bug workaround: https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/INSTALL.KUBEADM.md#issues--workarounds
    ovs-vsctl add-br br-int
}






# basic system setup
if test -n "$1"; then
    sudo hostnamectl set-hostname $1
else
    echo "Skipping setting hostname, no new hostname provided in arg1"
fi


# setup dnf caching
# great in theory, but GPG keys are not imported properly and it fails
# setup_dnf_local

sudo dnf remove -y zram-generator-defaults
sudo touch /etc/systemd/zram-generator.conf
echo "zram" | sudo tee /etc/modprobe.d/blacklist
sudo systemctl mask dev-zram0.device

sudo dnf install -y htop nload fish micro iproute-tc python3-pip openvswitch openvswitch-devel rsync make ovn ovn-host ovn-vtep ovn-central ldns-utils bpftrace python3-bcc tcpdump scapy python3-psutil NetworkManager-ovs NetworkManager-tui

# packages for building OVS
INSTALL_PKGS=" \
    python3-pyyaml bind-utils procps-ng openssl numactl-libs firewalld-filesystem \
    libpcap hostname util-linux\
    libunwind-devel libatomic \
    python3-pyOpenSSL \
    autoconf automake libtool g++ gcc fedora-packager rpmdevtools \
    unbound unbound-devel groff python3-sphinx graphviz openssl openssl-devel \
    checkpolicy libcap-ng-devel selinux-policy-devel systemtap-sdt-devel libbpf-devel libxdp-devel numactl-devel \
    iptables iproute iputils hostname unbound-libs kmod go" && \
sudo dnf install --best --refresh -y --setopt=tsflags=nodocs $INSTALL_PKGS

# install helper for copying files to the system
setup_sudo_rsync


# install container runtime
install_docker

# install Kubernetes
install_kubernetes


configure_systemd_networkd $1
# configure_network_manager $1

echo "KUBELET_EXTRA_ARGS=\"--node-ip=$(cluster_ip $1)\"" > /etc/sysconfig/kubelet
systemctl restart kubelet


# build OVN Kubernetes
cd $HOME
git clone https://github.com/ovn-org/ovn-kubernetes || true
cd ovn-kubernetes
# make -C go-controller
# sudo make -C go-controller install


# remove systemd-resolved before reboot
# sudo dnf remove -y systemd-resolved
# sudo rm -f /etc/resolv.conf
#echo "DNS=192.168.1.1" | sudo tee -a /etc/systemd/resolved.conf

sudo reboot
