\chapter{Experiment design}
\label{chap:design}

As we showed in \cref{chap:refs}, OVS inserts flow rules into datapaths on demand after upcalls. Therefore, some packets require much more processing and can slow the system down. Eelco Chaudron investigated the cost of an upcall in OVS\todo{link source} using eBPF probes in the Linux kernel. His experiments show that processing a packet through the slow path can take anywhere between \qty{700}{\us} and \qty{10}{\ms} extra compared to the fast path.

Based on this knowledge, we tried to answer the following questions about OVS internals:

\begin{itemize}
    \item When are flow rules removed from the datapaths? (\cref{design:flow-eviction})
    \item What is the cost of an upcall when measured from the user space? (\cref{design:upcall-cost})
    \item Which packets generate upcalls? (\cref{design:upcall-generators})
    \item What is the impact of artificial upcall-only traffic on the performance of the whole system? (\cref{design:upcall-impact})
\end{itemize}

The following sections describe our methods to answer the questions. The next chapter discusses our results.


% quote https://developers.redhat.com/articles/2022/02/07/investigating-cost-open-vswitch-upcalls-linux


\section{Removal of flow rules from datapaths}
\label{design:flow-eviction}

We can observe an effect of an upcall using the \ident{ping} tool. The first packet has higher latency than the rest of the ICMP packets.

\vspace{0.5cm}

\begin{lstlisting}[caption=Output of the \ident{ping} command in the virtualized environment, captionpos=b, basicstyle=\ttfamily\scriptsize]
[root@kb2 ~]# ping -c 5 kb3
PING kb3 (192.168.1.223) 56(84) bytes of data.
64 bytes from kb3 (192.168.1.223): icmp_seq=1 ttl=64 time=1.19 ms
64 bytes from kb3 (192.168.1.223): icmp_seq=2 ttl=64 time=0.404 ms
64 bytes from kb3 (192.168.1.223): icmp_seq=3 ttl=64 time=0.365 ms
64 bytes from kb3 (192.168.1.223): icmp_seq=4 ttl=64 time=0.424 ms
64 bytes from kb3 (192.168.1.223): icmp_seq=5 ttl=64 time=0.304 ms

--- kb3 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4062ms
rtt min/avg/max/mdev = 0.304/0.536/1.185/0.326 ms
\end{lstlisting}

The first packet delay is absent when we run the \ident{ping} command for the second time shortly after the first. The higher latency is visible again. This behavior can be explained by a flow rule eviction timeout that removes the rule from the datapath's forwarding table.

Assuming the timeout stays constant, we can measure it by varying the interval between ICMP packets. The dependency between the measured latency and the time delay should be constant except for one sharp increase in latency when the delays get longer than the timeout.

\paragraph{Our experiment}
We experimented in the following way:

\begin{enumerate}
    \item Generate a random number $D$ in the interval $\interval{8}{12}$
    \item sleep for $D$ seconds
    \item run \ident{ping -c 3 -i 0.01 192.168.1.221}
    \item log $D$ and both round trip times
    \item goto step 1 until we have enough
\end{enumerate}

We chose the interval based on non-rigorous preliminary experiments. After plotting RTT's dependence on the delay $D$, we expect to see:

\begin{itemize}
    \item The first RTT data points will form a line with a sharp increase at a value of $D$ corresponding to the eviction timeout

    \item The second RTT data points will form only a single horizontal line.

    \item The third ping will have the same results as the second.
\end{itemize}

Results of this experiment can be found in \cref{res:eviction-timeout}.

\section{Cost of an upcall}
\label{design:upcall-cost}

Eelco Chaudron's research into the cost of an upcall used eBPF to measure upcall cost directly in the kernel. However, as we saw in the previous section, upcalls have visible effects outside of the kernel.

The unchanged experiment devised in the previous section provides us with a direct measurement of the observable upcall effect. In each data point, we can subtract the two round-trip-times to get an insight into how much time upcall costs.

While this measurement methodology leads to less precise results than when directly measuring the in-kernel execution time using eBPF, we can use it to measure the upcall cost without special privileges on publicly deployed cloud hosting services.\todo{má smysl tohle zmiňovat, když to nezkusím? Napsal jsem do MetaCentra a domluvili jsme se, ze se mi ozvou. Ale zatim nic.}

Our results can be found in \cref{res:upcall-cost}.

\section{Packets generating upcalls}
\label{design:upcall-generators}

To stress-test the slow path, we have to be able to generate upcalls consistently. We have to find types of packets that will repeatedly miss all installed flow rules in the OVS datapath. We have a choice between a static analysis and a dynamic analysis of OVS.

We chose to approach the problem using dynamic analysis. We decided on dynamic analysis instead of static analysis because the results depend on OVS, the whole SDN, and its configuration. For static analysis, the search space is just too large. Also, the dynamic analysis tooling can be later reused on different OVS deployment than ours.

Our experiment sends varying packets in batches based on their type and monitors the upcalls and flow table changes using kprobes in the kernel and user statically-defined tracing (USDT) probes in \ident{ovs-vswitchd}.

The flow rules in OVS datapaths use the \ident{struct sw\_flow\_key}\todo{link} to represent the flow key. For every field of that structure, our measurement tool sends 1000 packets with the corresponding packet header field randomized and everything else fixed at arbitrary values.

We used Scapy to generate the packets. For example, the code to generate random TTL values in an IPv4 packet looks like this:

\begin{verbatim}
tag("IP(ttl)")
sendp(Ether(dst="aa:bb:cc:dd:ee:ff") / IP(dst="10.244.1.1", ttl=RandByte()), count=1000)
sleep(11)  # more than the eviction timeout
\end{verbatim}

Because we used dynamic analysis, we can not be certain that we covered all possible cases. We can draw some general conclusions from the measurement results and look into the source code for additional insights. However, there is always the possibility that we missed something.

\section{Impact of upcall-only traffic}
\label{design:upcall-impact}

\paragraph{Stress testing tool}
From the previous experiment, we learned that randomized unicast Ethernet source addresses generate upcalls. We used this knowledge to write a custom tool for sending minimalistic Ethernet frames. The Ethernet header needs only 14 bytes, but we used 16-byte packets due to slightly better performance. Instead of randomization, we filled the source Ethernet addresses with an increasing integer sequence.

Our tool is optimized for controlled and regular packet generation. Configured with a time interval between packets or a desired packet frequency, it tries to send packets as regularly as possible:

\begin{verbatim}
start_time = clock()
sent = 0
while True:
    now = clock()
    while we should have sent more packets than we sent:
        sent a new packet
        sent += 1
    
    sleep( until next packet is scheduled )
\end{verbatim}

Only instead of sending the packets one by one every time, we send them in batches using \ident{io\_uring} if it is possible. When we set the interval to \qty{1}{\ns} (zero is not possible due to division by zero), we can reach roughly 210k packets per second in a single thread on the dedicated test server.

\paragraph{The experiment}
For our experiment, we stressed OVS using our tool and monitored the system. We mainly measured OVS's memory consumption, the load average of the whole system and latencies observable from the \ident{victim} pod. We always started with a freshly restarted \ident{ovs-vswitchd}.

The results of the experiment can be found in \cref{res:upcall-stress}.