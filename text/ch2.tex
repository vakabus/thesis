\chapter{Experiment design}
\label{chap:env}

As we showed in the previous chapter, OVS inserts flow rules into datapaths on demand after upcalls. Therefore, some packets require much more processing and can slow the system down. Eelco Chaudron investigated the cost of an upcall in OVS\todo{link source} using eBPF probes in the Linux kernel. His experiments show that processing a packet through the slow-path can take anywhere between \qty{700}{\us} and \qty{10}{\ms} extra compared to the fast-path.

Based on this knowledge, we tried to answer the following questions about OVS internals:

\begin{itemize}
    \item When are flow rules removed from the datapaths? (\cref{design:flow-eviction})
    \item What is the cost of an upcall when measured from the user-space? (\cref{design:upcall-cost})
    \item Which packets generate upcalls? (\cref{design:upcall-generators})
    \item What is the impact of artificial upcall-only traffic on the performance of the whole system? (\cref{design:upcall-impact})
\end{itemize}

The following sections first discuss the hardware and software environment we used for our experiments. The sections after that describe our methods to answer the questions. The next chapter will discuss our results.


% quote https://developers.redhat.com/articles/2022/02/07/investigating-cost-open-vswitch-upcalls-linux

\section{Initial assumptions and experiments}
\label{sec:init-assum}

Flow rules in OVS are created by upcalls (see \cref{sec:ovs-internals}). 


\section{Software environment}
\label{sec:sw-env}

We run our experiments and measurements on a Kubernetes cluster with OVN-Kubernetes as the CNI plugin and Docker as the container runtime. OVN-Kubernetes was installed using containerized setup following the official installation guide\todo{link source}. We used Fedora 38 as the base Linux distribution. To allow reproducibility, we fully automated the installation procedure. Installation scripts with usage instructions are attached to this work.\todo{clean installation scripts and link them here}

In our experiments, we always used a three-node cluster with one node dedicated as a control node. We didn't test cluster installations configured for high availability (HA). We focused on the internals of OVS, a part of low-level networking infrastructure. We do not expect any significant difference between HA and non-HA clusters.

We named our cluster nodes \kb{1}, \kb{2}, and \kb{3}. The node \kb{1} is always the control node.

\section{Hardware environment}
\label{sec:hw-env}

We installed the experimental cluster in two different hardware configurations. One installation run virtualized on Proxmox VE. The other installation used dedicated hardware. When we write about an experiment and do not specify where it runs, we are presenting results from the cluster on dedicated hardware.

\paragraph{Virtualized installation}
We used the virtualized environment for development and debugging. The physical host was running Proxmox VE 7.4-3, and it was configured with an Intel\textsuperscript{\textregistered} Core\textsuperscript{TM} i7-3770 CPU running at 3.40 GHz with $4$ cores, $8$ threads, and $31.23$ GiB of RAM.

The virtual machines used for the cluster nodes were each configured with $4$ virtual cores and $4$ GiB of RAM. We had initially started experimenting with $2$ cores per node to prevent overprovisioning, but our workloads always fully stressed only one node (always \kb{2}), and we were mostly interested in system behavior with multiple threads.

The cluster nodes were equipped with $2$ virtual network interfaces connected to two Linux bridges on the host. We used one interface for system management and the other for cluster interconnect. We did not artificially limit the throughput or latency of the link between nodes. When measured between \kb{2} and \kb{3} using \ident{iperf3} with the default configuration and \ident{ping} with $100$ samples, the throughput was $14.5$ Gbps and the average round trip time $0.383$ ms.

\paragraph{Dedicated hardware}

We used the cluster with dedicated hardware to validate the results observed in the virtual environment. The nodes were Dell PowerEdge R730 servers, configured with Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} CPU E5-2690 v4 running at 2.60GHz with $14$ cores, $28$ threads, and $131$ GB of RAM.

Similar to the virtualized nodes, the dedicated nodes had 2 network interfaces. One management 1 GbE interface was connected to the Internet, and another cluster only 10 GbE interface was connected to a switch. We configured the switch to use VLANs to isolate the cluster traffic from all other ports.


\section{Removal of flow rules from datapaths}
\label{design:flow-eviction}

We can observe an effect of a upcall using the \ident{ping} tool. The first packet has higher latency than the rest of the ICMP packets.

\vspace{0.5cm}

\begin{lstlisting}[caption=Output of the \ident{ping} command in the virtualized environment, captionpos=b, basicstyle=\ttfamily\scriptsize]
[root@kb2 ~]# ping -c 5 kb3
PING kb3 (192.168.1.223) 56(84) bytes of data.
64 bytes from kb3 (192.168.1.223): icmp_seq=1 ttl=64 time=1.19 ms
64 bytes from kb3 (192.168.1.223): icmp_seq=2 ttl=64 time=0.404 ms
64 bytes from kb3 (192.168.1.223): icmp_seq=3 ttl=64 time=0.365 ms
64 bytes from kb3 (192.168.1.223): icmp_seq=4 ttl=64 time=0.424 ms
64 bytes from kb3 (192.168.1.223): icmp_seq=5 ttl=64 time=0.304 ms

--- kb3 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4062ms
rtt min/avg/max/mdev = 0.304/0.536/1.185/0.326 ms
\end{lstlisting}

The first packet delay is absent when we run the \ident{ping} command for the second time shortly after the first. The higher latency is visible again. This behavior can be explained by a flow rule eviction timeout that removes the rule from the datapath's forwarding table.

Assuming the timeout stays constant, we can measure it by varying the interval between ICMP packets. The dependency between the measured latency and the time delay should be constant except for one sharp increase in latency when the delays get longer than the timeout.

The implementation details of this experiment and our results can be found in \cref{res:eviction-timeout}.
\todo{shouldn't the implementation details be only here?}

\section{Cost of an upcall}
\label{design:upcall-cost}

We can also use the experiment devised in the previous section to measure the cost of an upcall and reproduce Eelco Chaudron's research with a different method. While this measurement methodology leads to less precise results than when directly measuring the in-kernel execution time using eBPF, we can use it to measure the upcall cost without special privileges on publicly deployed cloud hosting services.\todo{má smysl tohle zmiňovat, když to nezkusím? Napsal jsem pár provozovatelům, jestli náhodou neprovozují OVS a uvidíme}

Our results can be found in \cref{res:upcall-cost}.

\section{Packets generating upcalls}
\label{design:upcall-generators}

To stress-test the slow path, we have to be able to generate upcalls consistently. We have to find types of packets that will repeatedly miss all installed flow rules in the OVS datapath. We did a dynamic analysis and designed an experiment where we sent varying packets and monitored the upcalls and flow table changes. The flow rules in OVS datapaths use the \ident{sw\_flow\_key}\todo{link} structure to represent the flow key. For every field of that structure, our measurement tool sends 1000 packets with the corresponding packet header field randomized and everything else fixed at arbitrary values.

We decided on dynamic analysis instead of static analysis because the results depend on OVS, the whole SDN, and its configuration. We might be able to draw some general conclusions from the measurement results and look into the source code for additional insights. However, we are not sure we have found all packet types causing this. Additionally, our results only describe the behavior with Kubernetes, OVN-Kubernetes, and OVN (see \cref{sec:sw-env}).

...\todo{Dát sem zmínku o tom, že jsme už něco čekali kvůli macofu?}

\section{Impact of upcall-only traffic}
\label{design:upcall-impact}


Using the knowledge from the previous experiment, we wrote a tool for sending packets stressing the OVS's datapath. We then measured resource utilization and an impact on latency for different containers on the same Kubernetes node.
