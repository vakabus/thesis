\chapter{Experimental environment}
\label{chap:env}

This chapter describes the environment we used for running our experiments. The information provided here should allow anyone to replicate our measurements.

\section{Hardware}
\label{sec:hw-env}

For our research, we used two experimental cluster with different hardware configurations. One installation run virtualized on Proxmox VE with only a single physical host. The other installation used dedicated hardware. When we write about an experiment and do not specify where it runs, we are presenting results from the cluster on dedicated hardware. However, most of the measurements can be replicated in a virtualized environment without a significant impact on the outcome.

\paragraph{Virtualized installation}
We used the virtualized environment for development and debugging. The physical host was running Proxmox VE 7.4-3, and it was configured with an Intel\textsuperscript{\textregistered} Core\textsuperscript{TM} i7-3770 CPU running at 3.40 GHz with $4$ cores, $8$ threads, and $31.23$ GiB of RAM.

The virtual machines used for the cluster nodes were each configured with $4$ virtual cores and $4$ GiB of RAM. We had initially started experimenting with $2$ CPU cores per node to prevent overprovisioning, but our workloads always fully stressed only one node (always \kb{2}), and we were mostly interested in system behavior with multiple threads. The overprovisioning did not seem to cause any problems.

The cluster nodes were equipped with $2$ virtual network interfaces connected to two virtual Linux bridges on the host. We used one interface for system management and WAN access. Tthe other bridge was used for cluster interconnect. We did not artificially limit the throughput or latency of the link between nodes. When measured between \kb{2} and \kb{3} using \ident{iperf3} with the default configuration and \ident{ping} with $100$ samples, the throughput was $14.5$ Gbps and the average round trip time $0.383$ ms.

\paragraph{Dedicated hardware}

We used the cluster with dedicated hardware to validate the results observed in the virtual environment. The nodes were Dell PowerEdge R730 servers, configured with Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} CPU E5-2690 v4 running at 2.60GHz with $14$ cores, $28$ threads, and $131$ GB of RAM.

Similar to the virtualized nodes, the dedicated nodes had 2 network interfaces. One management 1 GbE interface was connected to the Internet, and another cluster only 10 GbE interface was connected to a switch. We configured the switch to use VLANs to isolate the cluster traffic from all other ports. While we did not have the opportunity to use a fully dedicated switch, the switch should have had enough capacity that it would not be a bottleneck. The average round trip time between \kb{2} and \kb{3} (\ident{ping -c 100}) was $0.104$ ms.

\section{Software environment}
\label{sec:sw-env}

We run our experiments and measurements on a Kubernetes cluster with OVN-Kubernetes as the CNI plugin and Docker as the container runtime. OVN-Kubernetes was installed using containerized setup following the official installation guide\todo{link source}. We used Fedora 38 as the base Linux distribution. To allow reproducibility, we fully automated the installation procedure. Installation scripts with usage instructions are attached to this work.\todo{clean installation scripts and link them here}

In our experiments, we always used a three-node cluster with one node dedicated as a control node. We didn't test cluster installations configured for high availability (HA). We focused on the internals of OVS, a part of low-level networking infrastructure. We do not expect any significant difference between HA and non-HA clusters.

We named our cluster nodes \kb{1}, \kb{2}, and \kb{3}. The node \kb{1} is always the control node, our experiments always run on \kb{2}.

\subsection{OVN-Kubernetes configuration}
\label{subsec:ovnkube-limits}

We deployed a fully containerized OVN-Kubernetes, meaning that even OVS was deployed in a privileged container. The default OVS container spec file contains \href{https://github.com/ovn-org/ovn-kubernetes/blob/master/dist/templates/ovs-node.yaml.j2\#L84-L90}{this resource limit}:

\begin{verbatim}
resources:
    requests:
        cpu: 100m
        memory: 300Mi
    limits:
        cpu: 200m
        memory: 400Mi
\end{verbatim}

We have manually removed this limit for most of our experiments. We have also tested with the limit active and we always explicitly mention it when it is the case.

\subsection{Kubernetes configuration}

We always deployed three different pods to the Kubernetes cluster. The spec files are attached to this work. The pods were:

\begin{itemize}
    \item \ident{arch} on \kb{2} -- a pod running the latest Arch Linux Docker image. We used this pod for the main part of our experiments.

    \item \ident{victim} on \kb{2} -- again, a pod with the latest Arch Linux image. We used it to measure the impact of our experiments on pods sharing the same node.

    \item \ident{reflector} on \kb{3} -- a privileged Arch Linux container with a custom raw-socket-based packet reflector and \ident{iptables} rule preventing the kernel from sending ICMP connection refused packets. The reflector swapped the Ethernet and IP addresses in the received packets and sent them out again.
\end{itemize}

\subsection{Experiment implementation}

We developed our experiments mainly in Rust. Everything is contained within one project in a tool we call the \ident{analyzer}. In the few cases when Rust was not the best language for the task, we embedded scripts in other languages (mainly Bash and Python) in the Rust executable itself. Our Rust code always provides an entry point.

The main reasoning behind the language and architecture choice was personal preferences and ease of distribution -- the ability to create a single statically-linked executable that would work almost anywhere.

\subsection{Clock synchronization}

All our experiments use Linux's \ident{CLOCK\_MONOTONIC} clock for timekeeping. Because we measure everything on a single node, \kb{2}, the clock is perfectly consistent across different containers. There could be small discrepancies due to synchronization between CPU cores and scheduling, but we are mostly concerned about larger time scales so we assume these discrepancies will not affect our results.

