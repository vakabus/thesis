\chapter{Discussion}
\label{chap:discussion}

\section{Possible OVS improvements}
\label{sec:improv}

Throughout our experiments, we have identified several areas in OVS that we believe could be immediately improved upon:

\begin{itemize}
    \item In in \ident{ovs-vswitchd}, instead of a fixed flow eviction timeout, enforce the flow limit by using a PID controller and a variable eviction timeout.
    \item Free unused memory
    \item Remove the resource limits in OVN-Kubernetes default container configuration
\end{itemize}

Other areas require better investigation before they can be improved, but we believe that it is worth the effort:

\begin{itemize}
    \item Gracefully handle out-of-memory conditions
    \item Investigate how best to prevent upcalls when using OVS as a firewall
\end{itemize}

\subsection{Flow limit with a PID controller}

In \cref{subsec:standard-behavior}, we explored how \ident{ovs-vswitchd} enforces the limit of the number of flows in the kernel flow table. In \cref{subsec:cpu-limit}, we followed up with additional details on how the limit is calculated. We believe that a PID controller would prevent the unstable oscillating behavior we saw in \cref{fig:packet-flood-limited}.

There are multiple possible implementations and we do not have enough data to say which one would work the best. We would suggest doing one of the following:

\begin{itemize}
    \item There could be a single PID loop taking the runtime of the revalidator threads as input. The output would be the current size limit of the flow table. Another PID loop would take the real number of flows in the flow table and output the flow eviction timeout.

    \item A single PID loop could generate the eviction timeout. An additional emergency mechanism would be needed to prevent accidental overfilling.
\end{itemize}

\subsection{Free unused memory}

In \cref{par:memory-usage}, we observed that memory is never released in \ident{ovs-vswitchd} after it has been allocated. It is technically not leaked, because \ident{ovs-vswitchd} will reuse the memory whenever there is a need. But under normal circumstances, there are not that many flows. 

While there could be reasons for not releasing the allocated memory (e.g. it would require synchronization), we think that it could be a problem in deployments on hardware with limited resources.

Alternatively, the maximum number of flows could be by default dependent on the amount of system memory. Defaulting to using 10\% of system memory at most seems as a better solution compared to allocating fixed 2GB.


\subsection{Default OVN-Kubernetes configuration}

In \cref{subsec:ovnkube-limits} we talked about the default resource limit of OVS deployment in OVN-Kubernetes. We suggest completely removing the limit. Unless the crashes and extremely high latencies can be prevented, limiting OVS's resources currently introduces more problems than it solves.

To limit memory usage without strict limits, default maximum size of the flow table could be decreased from 200k to a small fraction of it.


\subsection{Out-of-memory conditions}

In \cref{subsec:memory}, we observed crashes of \ident{ovs-vswitchd} when we limited the available physical memory. While we understand, that reacting to limited memory is not simple, we believe that a graceful degradation of functionality would be preferred when it comes to networking infrastructure. Moreover, there seems to be a correlation between the number of flows in the flow table and memory usage. It should be possible to limit the number of flows not only based on computation time but also based on the amount of available memory.

While appealing in theory, we think that a practical implementation would require better insight into how memory is allocated and used. Therefore improvement of this requires further research.

\subsection{OVS as a firewall}

In \cref{subsec:ethernet}, we have found a design flaw in OVS that prevents OVS from effectively blocking traffic. OVS allows only positive matches in its datapath flow rules. Negative wildcard matches can currently be handled only in userspace. Out of the issues we have discovered, we believe that this is the most complicated one to fix. 

\subsubsection{Fix without changing the datapath interface}
Changing the datapath interface might be undesirable because a similar interface is also implemented in hardware \cite{OVSHW}. Additionally, the hardware offloaded processing is very likely to have the same issue and we want to prevent upcall generation there as well. Therefore, we might want to generate ordinary datapath flow rules which would drop the invalid packets. This is however impossible without eliminating the possibility of a significant growth in the number of flow rules.

\paragraph{Exponential upper bound}
In general, let us assume an OpenFlow table with a default drop rule and several exact match rules accepting the packet (e.g. resubmitting it to a different table). Every rule matching a packet is equivalent to a logic formula where the variables represent bits of the flow key. Specifically, the formula is a conjunction. The whole table can be therefore expressed as a single DNF formula matching valid packets. Let us call the formula $F$.

Now, let us express the formula matching packets that should be dropped. $\neg F$ is true for all invalid packets. However, it is not in DNF anymore, so we cannot translate it directly back. Using De Morgan's laws, we can rewrite $\neg F$ as a CNF formula of the same length. Then, we have to convert it back to DNF. However, the conversion of a CNF formula to a DNF formula could potentially lead to exponential growth in the number of clauses. Therefore, a potentially exponential explosion in the number of flow rules we need to drop the packets.

\paragraph{Practical implementation}
It might still be practical to use this technique to implement simple port security and allow only a single MAC address on a switch port. That would require only 49 rules - 48 to drop invalid packets, a single rule for recirculation of valid packets. However, expressing anything more complicated is probably impractical due to drops in performance.

\subsubsection{Fix with a change in the datapath interface}

If we allow ourselves to change the datapath interface and in turn the kernel module, the fix can be implemented relatively easily. We could leave the processing as is and add packet filtering before the datapath makes an upcall. For example, in the kernel module, we could add a second, negative, flow table for upcall filtering. Any packet it would match would get dropped and not sent to the user space. The used flow key and most of the other data structures could stay the same, only the interface would have to include new commands for manipulating the negative flow table.


\section{Recommendation for public cloud providers}

Our findings certainly impact public cloud providers using OVS. We would recommend at the very least:

\begin{enumerate}
    \item Verify, that \ident{ovs-vswitchd} is running without any resource constraints.
    \item Monitor the number of flows in the flow tables (i.e. \ident{ovs-dpctl show}) and trigger a warning whenever the number of flows reaches the limit.
\end{enumerate}

In case the cloud provider uses OVS for firewalling, we would recommend switching to some other technology. OVS is in our opinion currently unsuitable for blocking undesirable traffic.


\section{Future work}

We suggested practical improvements to OVS in \cref{sec:improv}. There is also still space for more theoretical and experimental research, even though we do not know about any directions likely to yield significant results. We did not pursue investigations into the following areas and they may be a potential topic for additional research:

\begin{itemize}
    \item We have not verified the translation mechanism between OpenFlow and datapath flow rules in \ident{ovs-vswitchd} for optimality. There is a possibility that inefficiencies in the translation mechanism could be abused to generate upcalls similar to our results.

    \item Similarly, we have not explored what happens during network reconfiguration and whether the transition can cause performance issues.
\end{itemize}


